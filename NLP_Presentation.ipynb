{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7f986e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent vs. Sequence Processing Approaches\n",
    "\n",
    "### Understanding Context\n",
    "context of a text input is used to better interpret the true meaning \n",
    "\n",
    "Example: \"He went for a walk\" \n",
    "\n",
    "- Unknown context: interpret as a statement of fact  <br/>\n",
    "- Known context: interpret as an expression of frustration  <br/>\n",
    "- Context = walking as a way to cool off and clear their head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fa427",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Sequential Text Flow\n",
    "\n",
    "Information about natural order how text or speach is build up:\n",
    "\n",
    "- information: word positions and the order about the positions <br/>\n",
    "- left-to-right approach: for long sequences or during sequence generation <br/>\n",
    "- unidirectional problem: missing information until full sequence processed <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04abfe1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Sequential Text Flow - Example\n",
    "\n",
    "- \"I will be late for training...\"  <br/>\n",
    "- \"I will be late for training if I miss the train...\" <br/>\n",
    "- \"I will be late for training if I miss the train next week\" <br/>\n",
    "<br/>\n",
    "- \"I will be late for the train next week if I miss the training.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fb263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Sequential Text Flow - Challanges\n",
    "Challanges:<br/>\n",
    "- not easy to determine size of a sequence <br/>\n",
    "- relevance of words can strongly vary <br/>\n",
    "- hard to assign specific meaning/rule to a position in a sequence <br/>\n",
    "\n",
    "One approach is to use the idea of dynamic system modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3de4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Properties of Dynamic System Modeling\n",
    "\n",
    "- Approach 1: Finite Impulse Response (FIR), sequencal model (feedforward network) <br/>\n",
    "- Approach 2: Infinite Impulse Response (IIR), reccurent model (feedback network) <br/>\n",
    "\n",
    "Both approaches will be presented today.\n",
    "\n",
    "Direct processing:<br/>\n",
    "- limits the input values (length of input)\n",
    "- no problem with instability and oscillating\n",
    "Recurrent processing:<br/>\n",
    "- input vector/sequence in moderate size corresponing to its immediate neighbourhood (internal state)<br/>\n",
    "- can be instable and problem of oscillating<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ae5de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dynamic System Modeling - FIR\n",
    "\n",
    "Finite Impulse Response (FIR):\n",
    "It is the so called first order system, an example of which is a heat exchange process. \n",
    "After heating one end of a metal rod to a certain temperature, the temperature of the other end will change proportionally\n",
    "to the temperature difference of both ends. \n",
    "To a temperature impulse at one end, the other end’s temperature will follow similar like a stochastic moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206b8c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dynamic System Modeling - IIR\n",
    "\n",
    "Infinite Impulse Response (IIR):\n",
    "It is the so called second order system, an example of such system is a mass fixed on an elastic\n",
    "spring, an instance of which being the car and its suspension. \n",
    "Depending on the damping of the spring component, this system may oscillate or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04f98b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "- process sequences of arbitrary length, theoreticaly <br/>\n",
    "- unidirectional (typically left-to-right) <br/>\n",
    "- importand building block for natural language or audio processing applications <br/>\n",
    "- therefore solve problem of memory with modifications (most prominent LSTM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa23291",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "$h_t$ as hidden state, sematics of words already processed, dependent on current input, continously updated <br/>\n",
    "$x_t$ as word in input sequence as vector <br/>\n",
    "$y_t$ as vector output at particular position <br/>\n",
    "\n",
    "\n",
    "$h_t = H(h_{t-1}, x_{t})$  <br/> \n",
    "$y_t = Y(h_t)$ <br/>\n",
    "<br/>\n",
    "Often no notion of an external output besides the hidden state ($Y$ as identity function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af61e03",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "## Discrete Linear System Comparison\n",
    "\n",
    "$h_t$ as hidden state, sematics of words already processed <br/>\n",
    "$x_t$ as word in input sequence as vector <br/>\n",
    "$y_t$ as vector output at particular position <br/>\n",
    "\n",
    "Very similar to Discrete Linear Systems: <br/>\n",
    "$h_t = H(h_{t-1}, x_{t-1})$  <br/> \n",
    "- typically a non-linear func. in RNN and abritratry complex <br/> \n",
    "- RNN internal state is independent of $x_{t}$ and not $x_{t-1}$ <br/>\n",
    "<br/>\n",
    "$y_t = Y(h_t, x_t)$ <br/>\n",
    "- RNN feedtrough (non-dynamic influence) retained by $y_t$ depending on hidden state $h_t$ <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e97a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "$h_t = H(h_{t-1}, x_{t})$  <br/> \n",
    "$y_t = Y(h_t)$<br/>\n",
    "- weights, biases and activation function stay the same in all cells\n",
    "- changing input ($h_{t-n}, x_{t-n}$ per cell\n",
    "- additional feedback loop of hidden layer $h_{t-n}$\n",
    "\n",
    "<img src=\"NLP_NEF/NLP_NEF_1.PNG\" alt=\"RNN Cell \" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756be18f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "- In theory arbitrary length of sequence <br/>\n",
    "- In practice vanishing gradients due to \"chaining\" <br/>\n",
    "- Difficulties for long-term memory <br/>\n",
    "\n",
    "*In RNN the derivatives are recursively passed through the same neural network resulting that gradients will vanish*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ee7ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long Short-Term Memory (LSTM)\n",
    " \n",
    " - add additional state $c$ as support for long-term memory <br/>\n",
    " - replace RNN state vecotor $h$ with two state vector $h$,$c$ <br/>\n",
    "<br/>\n",
    "$h_t$ as hidden state, sematics of words already processed and vector output <br/>\n",
    "$x_t$ as word in input sequence as vector <br/>\n",
    "$c_t$ as memory state\n",
    " <br/>\n",
    "$$h_t = H(h_{t-1}, x_{t},c_{t})$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1})$$  <br/> \n",
    "$$y_t = h_t$$ <br/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d4990",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long Short-Term Memory (LSTM)\n",
    "$$h_t = H(h_{t-1}, x_{t},c_{t})$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1})$$  <br/> \n",
    "$$y_t = h_t$$ <br/>\n",
    "\n",
    "<img src=\"NLP_NEF/NLP_NEF_2.PNG\" alt=\"RNN Cell \" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03d18c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Updating Memory and Controlling Output with Gates\n",
    "\n",
    "- Forget Gate $f_t$: controls information to be neglected of previous memory state $c_{t-1}$ <br/>\n",
    "- Input Gate $i_t$:  controls information retriaval from the current input to current memory state $c_{t}$ <br/>\n",
    "- Output Gate $o_t$ controls output information is read from the memory state $c_{t}$ to the next cell  <br/>\n",
    "\n",
    "*0 = no pass through* <br/>\n",
    "*1 = full pass-through* <br/>\n",
    "\n",
    "\n",
    "$$h_t = y_t = H(h_{t-1}, x_{t},c_{t}) = o_t ○ tanh(c_t)$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1}) = f_t ○ c_{t-1} + i_t ○ C^*(h_{t-1},x_t)$$  <br/> \n",
    "$$C^*(h_{t-1},x_t) = tanh(W_{h_c} h_{t-1} + W_{x_c} X_t + b_c)$$\n",
    "\n",
    "$○$ = element-wise multiplication, Hadamard product  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a915f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Updating Memory and Controlling Output with Gates\n",
    "$$h_t = y_t = H(h_{t-1}, x_{t},c_{t}) = o_t ○ tanh(c_t)$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1}) = f_t ○ c_{t-1} + i_t ○ C^*(h_{t-1},x_t)$$  <br/> \n",
    "$$C^*(h_{t-1},x_t) = tanh(W_{h_c} h_{t-1} + W_{x_c} X_t + b_c)$$\n",
    "\n",
    "\n",
    "<img src=\"NLP_NEF/NLP_LSTM.PNG\" alt=\"LSTM Cell \" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f24924",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long Short-Term Memory - Sumup\n",
    "\n",
    "Where is the magic?\n",
    "- Saturated activation functions such as sigmoid [0,1] and tanh [0,1]\n",
    "- Prevents activation values from growing arbitrarily if passed through multiple layers\n",
    "- Instability only “shadowed” by saturation (Data Processing by Feedback Networks, convergence to some values is enforced)\n",
    "- Just one approach of LSTM, more to study e.g. Gated Recurrent Units (GRU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4481c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long Short-Term Memory - Sumup\n",
    "\n",
    "Problems/Disadvantages:\n",
    "- only information from previous positions can be accessed\n",
    "- left-to-right restriction can lead to wrong semantics due to missing context\n",
    "- bi-directional models could help (e.g combo left-to-right right-to-left)\n",
    "- suffer from the unfavorable mathematical properties, revival of sequence processing"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
