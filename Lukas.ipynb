{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf7ceda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multihead Attention\n",
    "\n",
    "- one attention head c can capture one relationship\n",
    "\n",
    "- in a sentence there are many relationships\n",
    "\n",
    "- multiple head's whith their on $W^Q$, $W^K$ and $W^V$\n",
    "\n",
    "- additional $W^O$ to combine heads\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2369ed8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"MultiHead.svg\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7476f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multihead Attention Math\n",
    "\n",
    "\n",
    "- $W^O$ with $ d_{model} \\times hd_v$\n",
    "\n",
    "- $W^{O}_h$ with $d_{model} \\times d_v$\n",
    "\n",
    "- $c_j$ with $1 \\times d_v$\n",
    "\n",
    "\n",
    "$c_j$ of each head has dimensionality $d_v$, will be denoted with $c_{hj}$ \n",
    "\n",
    "$$z_j = \\sum_{h=1}^H W^{O}_h c_{hj} = W^{O} \\cdot [c_{1j} ... c_{Hj}]^T  $$\n",
    "\n",
    "$z_j$ with $( d_{model} \\times hd_v) \\times (hd_v \\times 1) = d_{model} $\n",
    "\n",
    "$z$ with $ inputs \\times d_{model} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556acd09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([np.random.random_sample(8) for x in range(3)])\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "h_count = 3 # Header Count\n",
    "\n",
    "W_Q = np.random.random_sample((h_count,d_q, d_model))\n",
    "W_K = np.random.random_sample((h_count,d_q, d_model))\n",
    "W_V = np.random.random_sample((h_count,d_v, d_model))\n",
    "\n",
    "\n",
    "c_jh = np.zeros((x.shape[0], h_count, d_v))\n",
    "\n",
    "for hi in range(h_count):\n",
    "    k_stars = np.array([np.dot(W_K[hi], xi) for xi in x])\n",
    "    q_stars = np.array([np.dot(W_Q[hi], xi).transpose() for xi in x])\n",
    "    v_stars = np.array([np.dot(W_V[hi], xi) for xi in x])\n",
    "    \n",
    "    for j in range(x.shape[0]):\n",
    "        qj_star = q_stars[j]\n",
    "        all_gj = np.array([np.dot(qj_star, k_stars[i]) / np.sqrt(d_model) for i in range(x.shape[0])]) # 3x1\n",
    "        sum_g = np.sum(np.array([math.exp(all_gj[i]) for i in range(x.shape[0])]))\n",
    "        alpha_j = np.array([math.exp(all_gj[i]) / sum_g for i in range(x.shape[0])])\n",
    "        c_jh[j][hi] = np.sum([np.dot(alpha_j[i], v_stars[i]) for i in range(x.shape[0])], axis=0)\n",
    "\n",
    "W_O = np.random.random_sample((x.shape[0],d_model, h_count*d_v))\n",
    "\n",
    "z = np.zeros((x.shape[0], d_model))\n",
    "c = c_jh.reshape(x.shape[0], h_count * d_v)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(W_O[i], c[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0948ce1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pointwise Feedforward Network\n",
    "\n",
    "- each position same transformation (with same weights)\n",
    "\n",
    "- fully connected\n",
    "\n",
    "- $d_{inner} > d_{model}$\n",
    "\n",
    "- take up approx. $2/3$ of transformer parameters\n",
    "\n",
    "- might serve as key/value pair (https://arxiv.org/pdf/2012.14913.pdf)\n",
    "\n",
    "- $in = out =  d_{model}$\n",
    "\n",
    "\n",
    "$$PFF(z_j) = W_2 F(W_1 z_j + b_1 ) + b2   $$\n",
    "\n",
    "$$ F(x) = max(0,x) = Relu $$\n",
    "\n",
    "\n",
    "- So attention might not be everything you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a53a7bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "d_inner = d_model * 4\n",
    "\n",
    "W_1 = np.random.random_sample((d_inner, d_model))\n",
    "W_2 = np.random.random_sample((d_model, d_inner))\n",
    "b_1 = np.random.random_sample((d_inner))\n",
    "b_2 = np.random.random_sample((d_model))\n",
    "relu = np.zeros((d_inner))\n",
    "y = np.zeros((x.shape[0], d_model))\n",
    "for i in range(x.shape[0]):\n",
    "    hidden_layer = np.maximum(np.dot(W_1, z[i] ) + b_1, d_inner)\n",
    "    y[i] = np.dot(W_2, hidden_layer) + b_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de0bcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Residual Connection and Layer Optimization\n",
    "\n",
    "- aim to improve the converge of optimzation algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ded670",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Residual Connection\n",
    "\n",
    "Substitue $y = f(x,p)$ by $y = x + g(x,q)$ where q and p are parameter vectors\n",
    "\n",
    "This means that $g(x,q) = f(x,p) - x$\n",
    "\n",
    "- $ g(x,q)$ can be easier to optimize if f is close to the identity function $id(x) = x$\n",
    "- $ q(x,q)$ learns how much the input x needs to change\n",
    "\n",
    "- if $initial weights = 0$ without residual, then $output\\approx zerofunction$\n",
    "- if $initial weights = 0$ with residual, then $output \\approx identity(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc01641",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reasoning:\n",
    "\n",
    "- gradient of objective function E ( Error function) $E(y)$.\n",
    "- Chain-Rule\n",
    "$$ E'(y) = E'(y)\\cdot y'$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} = \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x}$$\n",
    "\n",
    "Without residual: y = Px\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x} = \\frac{\\delta E}{\\delta y}P$$\n",
    "\n",
    "With residual connection: y = x + Qx, I = Identy Matrix (derivate of x)\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x} = \\frac{\\delta E}{\\delta y}(I+Q)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0c828",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One more justification - Vanishing Gradient Problem\n",
    "\n",
    "- Deep (stacked) network with H layers\n",
    "\n",
    "$$ y_h = Ix + F_h(x) = (I + F_h)x $$\n",
    "\n",
    "- stack of layers 1,...H\n",
    "\n",
    "$$ y = ( \\prod_{h=1}^H (I + F_h) ) x $$\n",
    "\n",
    "\n",
    "$$ \\prod_{h=1}^H (I + F_h) = I + \\sum_{h\\leq H}F_h + \\sum_{i<j\\leq H}F_jF_i +... + \\prod_{h=H,..,1}F_h $$\n",
    "\n",
    "- non residual would correspond to last term:\n",
    "\n",
    "$$  \\prod_{h=H,..,1}F_h$$\n",
    "\n",
    "- size of gradient tends to decrease with chaining layers -> vanishing\n",
    "\n",
    "- residual connection contains term $\\sum_hF_h $ ( sum of outputs of invididual layers)\n",
    "\n",
    "- this prevents gradients from vanishing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127de4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,3))\n",
    "result = layers.MultiHeadAttention(key_dim=2, num_heads=2, use_bias=False, kernel_initializer='zeros')(inputs, inputs)\n",
    "#result = layers.Add()([inputs, result])\n",
    "model = keras.models.Model(inputs=inputs, outputs=result)\n",
    "test_input = tf.constant([[[1,2,3]]])\n",
    "result = model(test_input) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97016f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization\n",
    "\n",
    "### Reasons\n",
    "\n",
    "- Input variables might have different scaling\n",
    "\n",
    "- in a linear model scaling is accounted for by the pseudoinversion X'X\n",
    "\n",
    "- might lead to bad numerical conditioning of the inverse\n",
    "\n",
    "- in a stacked network, outputs could also hit problematic regions of nonlinear activation functions\n",
    "\n",
    "- Weight Matrices are unbound -> could also grow limitless\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2044df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- a priori knowledge of mean $m$ and variance $v$ of variable $z$ from test-set\n",
    "\n",
    "$$ \\hat{y} = \\frac{z-m}{\\sqrt{v}}$$\n",
    "\n",
    "\n",
    "- For fit measure E = MSE the gradients:\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta z} = \\frac {\\delta E}{\\delta y}\\frac{1}{\\sqrt{v}} [(1-\\frac {1}{H}) - (z-m)^2 \\frac{1}{Hv}] $$\n",
    "\n",
    "\n",
    "- bounderies for the norm of gradients and hessian matrix of second derivatives\n",
    "\n",
    "- batch normalization seems to make the mapping smoother\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee3df6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer Normalization\n",
    "\n",
    "- in batch normalization gradient of one sample depends on all other samples as well\n",
    "\n",
    "- layer normalization calculates mean and variance over feature dimension\n",
    "\n",
    "- Layer Normalization in Transformer: \n",
    "\n",
    "$$ \\hat{x} = \\frac{x -m}{\\sqrt{v^2}}$$\n",
    "\n",
    "Applied after Residual Connection of MultiHeadAttention and PFF:\n",
    "\n",
    ">$ z^*_j = LayerNorm(z_j + x_j)$ where $ z_j = MultiHeadAttt(x_j, x)$\n",
    "\n",
    "\n",
    ">$ y^*_j = LayerNorm(y_j + z^*_j)$ where $y_j = PFF(z^*_j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817f78b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"BatchLayerNorm.png\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76b52c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "beta = random.uniform(-1, 1)\n",
    "gamma = random.uniform(-1,1)\n",
    "epsilon = 0.00001\n",
    "\n",
    "v_i = x[0]\n",
    "mu = 1 / v_i.shape[0] * np.sum(v_i)\n",
    "omega_squared = 1 / v_i.shape[0] * np.sum((v_i - mu)**2)\n",
    "Layer_Norm = gamma * (v_i - mu) / (np.sqrt(omega_squared) + epsilon) + beta\n",
    "print(Layer_Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e908b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How it is done\n",
    "\n",
    "$$ LayerNorm(x) = \\gamma \\frac{x-m}{\\sqrt{v^2} + \\epsilon}+ \\beta $$\n",
    "> $ x \\in \\mathbb{R}^{d_{model}}$\n",
    "\n",
    "> $ mean = m = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} x_i$\n",
    "\n",
    "> $ v^2 = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} (x_i - m)^2$\n",
    "\n",
    "> $ \\epsilon$ and $\\beta$ are two learnable parameters\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
