{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4c8718",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# install RISE with https://rise.readthedocs.io/en/stable/installation.html\n",
    "#!pip3 install -U scikit-learn\n",
    "#!pip3 install -U RISE\n",
    "#!pip3 install -U matplotlib\n",
    "#!pip3 install -U numpy\n",
    "# all imports\n",
    "#!pip3 install -U tensorflow\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cceec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6: Specific Problem of Natural Language Processing\n",
    "\n",
    "## by Ziwei Chen, Stephan Nef, Lukas Bamert and Jan Grau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a89b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. Words to mathematical representation\n",
    "2. Embedding the problem into already learnt\n",
    "3. Transformer Encoder\n",
    "    1. Self-Attention\n",
    "    2. position-wise Feedforward Networks\n",
    "    3. Residucal connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69e37b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "### What is NLP?\n",
    "  The study of language interactions from a computational perspective, enabling the development of algorithms and models capable of natural language understanding and natural language generation \n",
    " \n",
    "### Application areas:\n",
    "- Extracting information from text \n",
    "- Translating or generating text \n",
    "- Full conversational systems(e.g.chatbots or personal assistants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277eb13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Words to mathematical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1415d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems of word to NLP\n",
    "- For a language processing system based on a real-valued representation (such as neural networks), all input data must first be transformed into a numerical representation\n",
    "-  Unlike numerical data or data that already have an inherent numerical structure, such as images where each pixel can be specified by a vector of values for each color channel (e.g. three values for RGB) \n",
    "<img src=\"P1.png\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bba99a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrete representation\n",
    "#### Transformation of text into a sequence of real-valued vectors\n",
    "\n",
    "1. A simple vector representation of a given word can be created by constructing a vocabulary $V$ which contains a list of all potential words.\n",
    "\n",
    "2. Each word $w_i$ to its index $i$ in the vocabulary. Like one-hot encoding vectors. \n",
    "\n",
    "$\\qquad w_i \\in V \\qquad  \\Longrightarrow \\qquad t_i = [o_{i1},...,o_{iv}]$\n",
    "- $ o_{ij} = 1 \\; for\\;  i = j;\\;and$\n",
    "- $ o_{ij} = 0 \\; for\\; i \\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3aa0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "          'Text of first Lukas.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three is new.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "X = vectorizer.fit_transform(corpus)# columns of X correspond to the result of this method\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())# transforming a new document according to learn vocabulary\n",
    "vectorizer.transform(['A new Lukas is three.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc558e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of discrete word transformation as calculated above\n",
    "- They only capture information about a word’s position in the vocabulary, not in each sentance, which can be arbitrary\n",
    "- They do not allow mathematical definition of similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744da4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributed representation\n",
    "\n",
    "- Capture the syntactic and semantic information of a word\n",
    "- Preserve semantic similarity (e.g. approximately, about)\n",
    "- Model their usage in different contexts (e.g. bear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a79365",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Co-occurrence matrix\n",
    "  Pairwise count values on how many times two words appear together in a specific context window.  \n",
    "  \n",
    "  \n",
    "**Example**  \n",
    "       *Corpus:*   \n",
    "        \"I like deep learning\"  \n",
    "        \"I like NLP\",  \n",
    "        \"I enjoy flying\"\n",
    "\n",
    "<img src=\"P2.png\" width=50% style=\"margin-left:auto; margin-right:auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee5102",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Words Embedding\n",
    "- Iterating over all words in the entire corpus.\n",
    "- Predicting the probability of a proximal word using that word.\n",
    "- Updating the gradient by learning\n",
    "    \n",
    "    $x_i = Mt_i$ \n",
    "    \n",
    "    In weight matrix $M(V*d)$, where $V$ is number of words and $d$ is embedding dimension.   \n",
    "    Embedding vector $x_i$ corresponds to the $i$-th row and $t_i$ is the one hot encoded vector for word $w_i$.\n",
    "    \n",
    "    $x_i^\\star= x_i +p_j$\n",
    "    \n",
    "    Additional learned vector $p_j$ (or position embedding), same words occur in different position, to find out the relative position by learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c6680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Semantic Similarity    \n",
    "    Semantic-focused natural language processing needs to have the ability to identify which semantic representations are close to each other.\n",
    "#### Mathematical represent semantic similarity:\n",
    "$\\qquad w_{AB} = v_A'v_B$   \n",
    "\n",
    "    Where word A is encoded by vector $v_A$ and word B by vector $v_B$, their similarity can be measure by their vector product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945157d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example of Projection:\n",
    "<img src=\"P3.png\" width=20% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "$v_{B|A} = \\frac{v_Av_A'}{||v_A||}v_B = \\frac{v_Av_A'}{v_A'v_A}v_B = v_A \\frac{v_A'v_B}{||v_A||} = \\frac{v_A}{||v_A||}w_{AB} $  \n",
    "\n",
    " **The normalized cosine similarity:**   \n",
    "$ w_{AB}^\\star =\\frac{ v_A'v_B }{||v_A|| ||v_B||} = \\frac{ v_A'v_B }{v_A'v_Av_Bv_B'}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e9aa69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference Messi vs Lewandowski:  0.5662365\n",
      "0.1945092\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"Honestly, I don’t understand anything anymore\", \n",
    "             \"Miller said.\",\n",
    "             \"With all due respect to Messi and the other great players named,\",\n",
    "             \"no one deserved it as much as Lewandowski.\", \n",
    "             \"To be as remarkable as the Bavarian striker.\",\n",
    "             \"Lewandowski numbers do look better on paper\"]\n",
    "\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences=texts, vector_size=8, window=5, min_count=1, workers=4)\n",
    "vector_word1 = model.wv['Messi']\n",
    "vector_word2 = model.wv['Lewandowski']\n",
    "vector_word3 = model.wv['Miller']\n",
    "cos_sim1 = np.dot(vector_word1, vector_word2)/(np.linalg.norm(vector_word1)*np.linalg.norm(vector_word2))\n",
    "cos_sim2 = np.dot(vector_word2, vector_word3)/(np.linalg.norm(vector_word2)*np.linalg.norm(vector_word3))\n",
    "print(\"Difference Messi vs Lewandowski: \",cos_sim1)\n",
    "print(cos_sim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e919d50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent vs. Sequence Processing Approaches\n",
    "\n",
    "### Understanding Context\n",
    "context of a text input is used to better interpret the true meaning \n",
    "\n",
    "Example: \"He went for a walk\" \n",
    "\n",
    "- Unknown context: interpret as a statement of fact  <br/>\n",
    "- Known context: interpret as an expression of frustration  <br/>\n",
    "- Context = walking as a way to cool off and clear their head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf9dac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Sequential Text Flow\n",
    "\n",
    "Information about natural order how text or speach is build up:\n",
    "\n",
    "- information: word positions and the order about the positions <br/>\n",
    "- left-to-right approach: for long sequences or during sequence generation <br/>\n",
    "- unidirectional problem: missing information until full sequence processed <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561952d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Sequential Text Flow - Example\n",
    "\n",
    "- \"I will be late for training...\"  <br/>\n",
    "- \"I will be late for training if I miss the train...\" <br/>\n",
    "- \"I will be late for training if I miss the train next week\" <br/>\n",
    "<br/>\n",
    "- \"I will be late for the train next week if I miss the training.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03edea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Sequential Text Flow - Challenges\n",
    "Challenges:<br/>\n",
    "- not easy to determine size of a sequence <br/>\n",
    "- relevance of words can strongly vary <br/>\n",
    "- hard to assign specific meaning/rule to a position in a sequence <br/>\n",
    "\n",
    "One approach is to use the idea of dynamic system modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b5905",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Properties of Dynamic System Modeling\n",
    "\n",
    "- Approach 1: Finite Impulse Response (FIR), sequencal model (feedforward network) <br/>\n",
    "- Approach 2: Infinite Impulse Response (IIR), reccurent model (feedback network) <br/>\n",
    "\n",
    "Both approaches will be presented today.\n",
    "\n",
    "Direct processing:<br/>\n",
    "- limits the input values (length of input)\n",
    "- no problem with instability and oscillating \n",
    "\n",
    "\n",
    "Recurrent processing:<br/>\n",
    "- input vector/sequence in moderate size corresponing to its immediate neighbourhood (internal state)<br/>\n",
    "- can be instable and problem of oscillating<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d63d70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dynamic System Modeling - FIR\n",
    "\n",
    "Finite Impulse Response (FIR):\n",
    "It is the so called first order system, an example of which is a heat exchange process. \n",
    "After heating one end of a metal rod to a certain temperature, the temperature of the other end will change proportionally\n",
    "to the temperature difference of both ends. \n",
    "To a temperature impulse at one end, the other end’s temperature will follow similar like a stochastic moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffcfbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dynamic System Modeling - IIR\n",
    "\n",
    "Infinite Impulse Response (IIR):\n",
    "It is the so called second order system, an example of such system is a mass fixed on an elastic\n",
    "spring, an instance of which being the car and its suspension. \n",
    "Depending on the damping of the spring component, this system may oscillate or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fbf36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "- process sequences of arbitrary length, theoreticaly <br/>\n",
    "- unidirectional (typically left-to-right) <br/>\n",
    "- importand building block for natural language or audio processing applications <br/>\n",
    "- therefore solve problem of memory with modifications (most prominent LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d22c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "$h_t$ as hidden state, sematics of words already processed, dependent on current input, continously updated <br/>\n",
    "$x_t$ as word in input sequence as vector <br/>\n",
    "$y_t$ as vector output at particular position <br/>\n",
    "\n",
    "$h_t = H(h_{t-1}, x_{t})$  <br/> \n",
    "$y_t = Y(h_t)$<br/>\n",
    "- weights, biases and activation function stay the same in all cells\n",
    "- changing input ($h_{t-n}, x_{t-n}$) per cell\n",
    "- additional feedback loop of hidden layer $h_{t-n}$\n",
    "\n",
    "<img src=\"NLP_NEF_1.PNG\" alt=\"RNN Cell \" width=50% style=\"margin-left:auto; margin-right:auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5b4f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "## Discrete Linear System Comparison\n",
    "\n",
    "$h_t$ as hidden state, sematics of words already processed <br/>\n",
    "$x_t$ as word in input sequence as vector <br/>\n",
    "$y_t$ as vector output at particular position <br/>\n",
    "\n",
    "Very similar to Discrete Linear Systems: <br/>\n",
    "$h_t = H(h_{t-1}, x_{t-1})$  <br/> \n",
    "- typically a non-linear func. in RNN and abritratry complex <br/> \n",
    "- RNN internal state is independent of $x_{t}$ and not $x_{t-1}$ <br/>\n",
    "<br/>\n",
    "$y_t = Y(h_t, x_t)$ <br/>\n",
    "- RNN feedtrough (non-dynamic influence) retained by $y_t$ depending on hidden state $h_t$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa02a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "- In theory arbitrary length of sequence <br/>\n",
    "- Difficulties with long-range dependencies due to vanishing gradients<br/>\n",
    "\n",
    "*In RNN the derivatives are recursively passed through the same neural network resulting that gradients will vanish*\n",
    "\n",
    "|> image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9255f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long-Short-Term Memory (LSTM)\n",
    " \n",
    " - add additional state $c$ as support for long-term memory <br/>\n",
    " - replace RNN state vector $h$ with two state vectors $h$,$c$ <br/>\n",
    "<br/>\n",
    "$h_t$ as hidden state, sematics of words already processed and vector output <br/>\n",
    "$x_t$ as word in input sequence as vector <br/>\n",
    "$c_t$ as memory state\n",
    " <br/>\n",
    "$$h_t = H(h_{t-1}, x_{t},c_{t})$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1})$$  <br/> \n",
    "$$y_t = h_t$$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0539896",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long-Short-Term Memory (LSTM)\n",
    "$$h_t = H(h_{t-1}, x_{t},c_{t})$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1})$$  <br/> \n",
    "$$y_t = h_t$$ <br/>\n",
    "\n",
    "<img src=\"NLP_NEF_2.PNG\" alt=\"RNN Cell \" width=60% style=\"margin-left:auto; margin-right:auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b9fb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Updating Memory and Controlling Output with Gates\n",
    "\n",
    "- Forget Gate $f_t$: controls information to be neglected of previous memory state $c_{t-1}$ <br/>\n",
    "- Input Gate $i_t$:  controls information retriaval from the current input to current memory state $c_{t}$ <br/>\n",
    "- Output Gate $o_t$ controls output information is read from the memory state $c_{t}$ to the next cell  <br/>\n",
    "\n",
    "*0 = no pass through* <br/>\n",
    "*1 = full pass-through* <br/>\n",
    "\n",
    "\n",
    "$$h_t = y_t = H(h_{t-1}, x_{t},c_{t}) = o_t \\circ tanh(c_t)$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1}) = f_t \\circ c_{t-1} + i_t \\circ C^*(h_{t-1},x_t)$$  <br/> \n",
    "$$C^*(h_{t-1},x_t) = tanh(W_{h_c} h_{t-1} + W_{x_c} X_t + b_c)$$\n",
    "\n",
    "$\\circ$ = element-wise multiplication, Hadamard product  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2add5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Updating Memory and Controlling Output with Gates\n",
    "$$h_t = y_t = H(h_{t-1}, x_{t},c_{t}) = o_t \\circ tanh(c_t)$$  <br/> \n",
    "$$c_t = C(h_{t-1}, x_{t},c_{t-1}) = f_t \\circ c_{t-1} + i_t \\circ C^*(h_{t-1},x_t)$$  <br/> \n",
    "$$C^*(h_{t-1},x_t) = tanh(W_{h_c} h_{t-1} + W_{x_c} X_t + b_c)$$\n",
    "\n",
    "\n",
    "<img src=\"NLP_LSTM.PNG\" alt=\"LSTM Cell \" width=60% style=\"margin-left:auto; margin-right:auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55268f08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long Short-Term Memory - Sumup\n",
    "\n",
    "Where is the magic?\n",
    "- Saturated activation functions such as sigmoid [0,1] and tanh [0,1]\n",
    "- Prevents activation values from growing arbitrarily if passed through multiple layers\n",
    "- Instability only “shadowed” by saturation (Data Processing by Feedback Networks, convergence to some values is enforced)\n",
    "- Just one approach of LSTM, more to study e.g. Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e6807",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Long Short-Term Memory - Sumup\n",
    "\n",
    "Problems/Disadvantages:\n",
    "- only information from previous positions can be accessed\n",
    "- left-to-right restriction can lead to wrong semantics due to missing context\n",
    "- suffer from the unfavorable mathematical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e5661",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One last thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944eabd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Transformer_Base.png\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d926b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Encoder.PNG\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6ff56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"attention.jpg\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb23dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember the problem of FIR filters? The length of the filter is too short for the actual input and quality of the data of the input may differ.\n",
    "\n",
    "There is a solution to this: $\\underline{attention}$.\n",
    "\n",
    "Let $v = [v_1,..., v_n]$ be a sequence of input vectors.\n",
    "\n",
    "Then we can define a context vector $c$ as $c= \\sum_{i=1}^n \\alpha_iv_i$.\n",
    "\n",
    "This can be extrapolated to different context vectors, each describing different contexts $j$:\n",
    "\n",
    "$$c_j = \\sum_{i=1}^n \\alpha_{ji}v_i $$\n",
    "\n",
    "\n",
    "where $\\alpha_{ji}$ is an attention weight from input $i$ to output $j$. A good way to achieve this is to use the softmax function:\n",
    "\n",
    "$$\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$$\n",
    "\n",
    "where $g_{ji}$ is using an alignment model to tell about the similarity of two vectors:\n",
    "\n",
    "$$ g_{ji} = \\frac{q'k_i}{\\sqrt{k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a17b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Attention1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057c84b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a context vector calculation in pratice\n",
    "\n",
    "# let's try to figure out the market value of newcomer _\n",
    "# given our scouting DB with current market values of known players\n",
    "\n",
    "v = {}\n",
    "v[\"messi\"] = 80\n",
    "v[\"lewandowski\"] = 40\n",
    "v[\"miller\"] = -25\n",
    "\n",
    "# since we already have an example of cosine similarity g_ji is given here\n",
    "your_player = \"ronald\"\n",
    "\n",
    "g = {}\n",
    "g[\"yp-messi\"] = 0.8\n",
    "g[\"yp-lewandowski\"] = 0.5\n",
    "g[\"yp-miller\"] = -0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffffdf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# a litte helper\n",
    "sum_eg = 0\n",
    "for mv in g:\n",
    "    sum_eg += math.exp(g[mv])\n",
    "\n",
    "# calculate attention values\n",
    "alpha_yp_messi = math.exp(g[\"yp-messi\"])/sum_eg\n",
    "alpha_yp_lewandowski = math.exp(g[\"yp-lewandowski\"])/sum_eg\n",
    "alpha_yp_maguire = math.exp(g[\"yp-miller\"])/sum_eg\n",
    "c = alpha_yp_messi *v[\"messi\"] + alpha_yp_lewandowski * v[\"lewandowski\"] + alpha_yp_maguire * v[\"miller\"]\n",
    "print(\"expected market value of \", your_player, \": \", round(c,2), \"Mio CHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takeaway\n",
    "\n",
    "We calculate basic attention by querying (your player aka $q$) to keys (Messi & co, aka $k$) to get a value (market value aka $v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb7f02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "What if we try to calculate our $q$, $k$, and $v$ by ourselves?\n",
    "\n",
    "This process is called self-attention, where each input vector from $x = [x_1,...,x_n]$ is also query, key and value:\n",
    "\n",
    "$$x_i = q_i = k_i = v_i$$\n",
    "\n",
    "However instead of just calculating $c_j = \\sum_{i=1}^n \\alpha_{ji}v_i$ with $\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$, it has been proven mathematically beneficial to linearly project these vectors (in our example we had a scalar value) into smaller dimensionalities. For this we use three projection matrices $W^Q$, $W^K$, $W^V$. This will give us the following equations:\n",
    "\n",
    "$$ q_i^\\star = W^Qq_i, k_i^\\star = W^Kk_i,v_i^\\star = W^Vv_i$$\n",
    "\n",
    "\n",
    "These $W$ play an essential role in the learning. Since the attention mechanism does not contain trainable parameters. Therefore given fixed inputs vectors, we need to learn the elements of the $W$'s. Also note that having two different matrices $W^Q$ and $W^K$ we will have asymetric relationships between the input vector elements.\n",
    "\n",
    "In the end we can calculate $c_j =  \\sum_{i=1}^n \\alpha_{ji}v_i^\\star$ for each element $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ee2c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Self-Attention1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347d03f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"SelfAttention_Context1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c54e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Self-Attention-Overall.drawio.png\" width=50% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eeca23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"Honestly, I don’t understand anything anymore\", \n",
    "             \"Miller said.\",\n",
    "             \"With all due respect to Messi and the other great players named,\",\n",
    "             \"no one deserved it as much as Lewandowski.\", \n",
    "             \"To be as remarkable as the Bavarian striker.\",\n",
    "             \"Lewandowski numbers do look better on paper\"]\n",
    "\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences=texts, vector_size=8, window=5, min_count=1, workers=4)\n",
    "x_messi = model.wv['Messi']\n",
    "x_lewa = model.wv['Lewandowski']\n",
    "x_miller = model.wv['Miller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b60adb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([x_messi, x_lewa, x_miller])\n",
    "\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "\n",
    "# generate the three projections matrices\n",
    "W_Q = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_K = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_V = np.random.random_sample((d_v, d_model)) # in a trainable model, those would be trained instead of random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417439dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "c = np.zeros((x.shape[0], d_v))\n",
    "\n",
    "\n",
    "k_stars = np.array([np.dot(W_K, xi) for xi in x])\n",
    "q_stars = np.array([np.dot(W_Q, xi).transpose() for xi in x])\n",
    "v_stars = np.array([np.dot(W_V, xi) for xi in x])\n",
    "\n",
    "for j in range(x.shape[0]):\n",
    "    qj_star = q_stars[j]\n",
    "    all_gj = np.array([np.dot(qj_star, k_stars[i]) / np.sqrt(d_model) for i in range(x.shape[0])]) # 3x1\n",
    "    sum_g = np.sum(np.array([math.exp(all_gj[i]) for i in range(x.shape[0])]))\n",
    "    alpha_j = np.array([math.exp(all_gj[i]) / sum_g for i in range(x.shape[0])])\n",
    "    c[j] = np.sum([np.dot(alpha_j[i], v_stars[i]) for i in range(x.shape[0])], axis=0)\n",
    "print(c[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e60ddd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Encoder.PNG\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb66040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multihead Attention\n",
    "\n",
    "- one attention head c can capture one relationship\n",
    "\n",
    "- in a sentence there are many relationships\n",
    "\n",
    "- multiple head's whith their on $W^Q$, $W^K$ and $W^V$\n",
    "\n",
    "- additional $W^O$ to combine heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545b6df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"MultiHead.svg\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f057f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multihead Attention Math\n",
    "\n",
    "\n",
    "- $W^O$ with $ d_{model} \\times hd_v$\n",
    "\n",
    "- $W^{O}_h$ with $d_{model} \\times d_v$\n",
    "\n",
    "- $c_j$ with $1 \\times d_v$\n",
    "\n",
    "\n",
    "$c_j$ of each head has dimensionality $d_v$, will be denoted with $c_{hj}$ \n",
    "\n",
    "$$z_j = \\sum_{h=1}^H W^{O}_h c_{hj} = W^{O} \\cdot [c_{1j} ... c_{Hj}]^T  $$\n",
    "\n",
    "$z_j$ with $( d_{model} \\times hd_v) \\times (hd_v \\times 1) = d_{model} $\n",
    "\n",
    "$z$ with $ inputs \\times d_{model} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeec2e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([np.random.random_sample(8) for x in range(3)])\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "h_count = 3 # Header Count\n",
    "\n",
    "W_Q = np.random.random_sample((h_count,d_q, d_model))\n",
    "W_K = np.random.random_sample((h_count,d_q, d_model))\n",
    "W_V = np.random.random_sample((h_count,d_v, d_model))\n",
    "\n",
    "\n",
    "c_jh = np.zeros((x.shape[0], h_count, d_v))\n",
    "\n",
    "for hi in range(h_count):\n",
    "    k_stars = np.array([np.dot(W_K[hi], xi) for xi in x])\n",
    "    q_stars = np.array([np.dot(W_Q[hi], xi).transpose() for xi in x])\n",
    "    v_stars = np.array([np.dot(W_V[hi], xi) for xi in x])\n",
    "    \n",
    "    for j in range(x.shape[0]):\n",
    "        qj_star = q_stars[j]\n",
    "        all_gj = np.array([np.dot(qj_star, k_stars[i]) / np.sqrt(d_model) for i in range(x.shape[0])]) # 3x1\n",
    "        sum_g = np.sum(np.array([math.exp(all_gj[i]) for i in range(x.shape[0])]))\n",
    "        alpha_j = np.array([math.exp(all_gj[i]) / sum_g for i in range(x.shape[0])])\n",
    "        c_jh[j][hi] = np.sum([np.dot(alpha_j[i], v_stars[i]) for i in range(x.shape[0])], axis=0)\n",
    "\n",
    "W_O = np.random.random_sample((x.shape[0],d_model, h_count*d_v))\n",
    "  \n",
    "z = np.zeros((x.shape[0], d_model))\n",
    "c = c_jh.reshape(x.shape[0], h_count * d_v)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(W_O[i], c[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8790ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pointwise Feedforward Network\n",
    "\n",
    "- each position same transformation (with same weights)\n",
    "\n",
    "- fully connected\n",
    "\n",
    "- $d_{inner} > d_{model}$\n",
    "\n",
    "- take up approx. $2/3$ of transformer parameters\n",
    "\n",
    "- might serve as key/value pair (https://arxiv.org/pdf/2012.14913.pdf)\n",
    "\n",
    "- $in = out =  d_{model}$\n",
    "\n",
    "\n",
    "$$PFF(z_j) = W_2 F(W_1 z_j + b_1 ) + b2   $$\n",
    "\n",
    "$$ F(x) = max(0,x) = Relu $$\n",
    "\n",
    "\n",
    "- So attention might not be everything you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5d518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "d_inner = d_model * 4\n",
    "\n",
    "W_1 = np.random.random_sample((d_inner, d_model))\n",
    "W_2 = np.random.random_sample((d_model, d_inner))\n",
    "b_1 = np.random.random_sample((d_inner))\n",
    "b_2 = np.random.random_sample((d_model))\n",
    "relu = np.zeros((d_inner))\n",
    "y = np.zeros((x.shape[0], d_model))\n",
    "for i in range(x.shape[0]):\n",
    "    hidden_layer = np.maximum(np.dot(W_1, z[i] ) + b_1, d_inner)\n",
    "    y[i] = np.dot(W_2, hidden_layer) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509cbf79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Residual Connection and Layer Normalization\n",
    "\n",
    "- aim to improve the converge of optimzation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd695914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Residual Connection\n",
    "\n",
    "Substitue $y = f(x,p)$ by $y = x + g(x,q)$ where q and p are parameter vectors\n",
    "\n",
    "This means that $g(x,q) = f(x,p) - x$\n",
    "\n",
    "- $ g(x,q)$ can be easier to optimize if f is close to the identity function $id(x) = x$\n",
    "- $ q(x,q)$ learns how much the input x needs to change\n",
    "\n",
    "- if $initial weights = 0$ without residual, then $output\\approx zerofunction$\n",
    "- if $initial weights = 0$ with residual, then $output \\approx identity(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1178e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reasoning:\n",
    "\n",
    "- gradient of objective function E ( Error function) $E(y)$.\n",
    "- Chain-Rule\n",
    "$$ E'(y) = E'(y)\\cdot y'$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} = \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x}$$\n",
    "\n",
    "Without residual: y = Px\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x} = \\frac{\\delta E}{\\delta y}P$$\n",
    "\n",
    "With residual connection: y = x + Qx, I = Identy Matrix (derivate of x)\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x} = \\frac{\\delta E}{\\delta y}(I+Q)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef69766",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One more justification - Vanishing Gradient Problem\n",
    "\n",
    "- Deep (stacked) network with H layers\n",
    "\n",
    "$$ y_h = Ix + F_h(x) = (I + F_h)x $$\n",
    "\n",
    "- stack of layers 1,...H\n",
    "\n",
    "$$ y = ( \\prod_{h=1}^H (I + F_h) ) x $$\n",
    "\n",
    "\n",
    "$$ \\prod_{h=1}^H (I + F_h) = I + \\sum_{h\\leq H}F_h + \\sum_{i<j\\leq H}F_jF_i +... + \\prod_{h=H,..,1}F_h $$\n",
    "\n",
    "- non residual would correspond to last term:\n",
    "\n",
    "$$  \\prod_{h=H,..,1}F_h$$\n",
    "\n",
    "- size of gradient tends to decrease with chaining layers -> vanishing\n",
    "\n",
    "- residual connection contains term $\\sum_hF_h $ ( sum of outputs of invididual layers)\n",
    "\n",
    "- this prevents gradients from vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e201cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,3))\n",
    "result = layers.MultiHeadAttention(key_dim=2, num_heads=2, use_bias=False, kernel_initializer='zeros')(inputs, inputs)\n",
    "result = layers.Add()([inputs, result])\n",
    "model = keras.models.Model(inputs=inputs, outputs=result)\n",
    "test_input = tf.constant([[[1,2,3]]])\n",
    "result = model(test_input) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45990a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization\n",
    "\n",
    "- Input variables have different scaling\n",
    "\n",
    "\n",
    "### Why is that an issue?\n",
    "\n",
    "- in a linear model scaling is accounted for by the pseudoinversion X'X\n",
    "\n",
    "- might lead to bad numerical conditioning of the inverse\n",
    "\n",
    "- in a stacked network, outputs could also hit problematic regions of nonlinear activation functions\n",
    "\n",
    "- Weight Matrices are unbound -> could also grow limitless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53969aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"BatchLayerNorm.png\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775b7f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- a priori knowledge of mean $m$ and variance $v$ of variable $z$ from test-set\n",
    "\n",
    "$$ \\hat{y} = \\frac{z-m}{\\sqrt{v}}$$\n",
    "\n",
    "\n",
    "- For fit measure E = MSE the gradients:\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta z} = \\frac {\\delta E}{\\delta y}\\frac{1}{\\sqrt{v}} [(1-\\frac {1}{H}) - (z-m)^2 \\frac{1}{Hv}] $$\n",
    "\n",
    "\n",
    "- bounderies for the norm of gradients and hessian matrix of second derivatives\n",
    "\n",
    "- batch normalization seems to make the mapping smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174fa19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer Normalization\n",
    "\n",
    "- in batch normalization gradient of one sample depends on all other samples as well\n",
    "\n",
    "- layer normalization calculates mean and variance over feature dimension\n",
    "\n",
    "- Layer Normalization in Transformer: \n",
    "\n",
    "$$ \\hat{x} = \\frac{x -m}{\\sqrt{v^2}}$$\n",
    "\n",
    "Applied after Residual Connection of MultiHeadAttention and PFF:\n",
    "\n",
    ">$ z^*_j = LayerNorm(z_j + x_j)$ where $ z_j = MultiHeadAttt(x_j, x)$\n",
    "\n",
    "\n",
    ">$ y^*_j = LayerNorm(y_j + z^*_j)$ where $y_j = PFF(z^*_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481fc63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "beta = random.uniform(-1, 1)\n",
    "gamma = random.uniform(-1,1)\n",
    "epsilon = 0.00001\n",
    "\n",
    "v_i = x[0]\n",
    "mu = 1 / v_i.shape[0] * np.sum(v_i)\n",
    "omega_squared = 1 / v_i.shape[0] * np.sum((v_i - mu)**2)\n",
    "Layer_Norm = gamma * (v_i - mu) / (np.sqrt(omega_squared) + epsilon) + beta\n",
    "print(Layer_Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba224e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How it is done\n",
    "\n",
    "$$ LayerNorm(x) = \\gamma \\frac{x-m}{\\sqrt{v^2} + \\epsilon}+ \\beta $$\n",
    "> $ x \\in \\mathbb{R}^{d_{model}}$\n",
    "\n",
    "> $ mean = m = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} x_i$\n",
    "\n",
    "> $ v^2 = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} (x_i - m)^2$\n",
    "\n",
    "> $ \\epsilon$ and $\\beta$ are two learnable parameters\n",
    "\n",
    "- Simply normalizing to zero mean and unity variance could constrain the inputs to a specific subset of the activation function\n",
    "\n",
    "- $\\epsilon$ and $\\beta$ can help learn the indentity function by scaling and shifting the normalized value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8071483",
   "metadata": {},
   "source": [
    "<img src=\"attention.jpg\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
