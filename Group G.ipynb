{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c8718",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# install RISE with https://rise.readthedocs.io/en/stable/installation.html\n",
    "#!pip3 install -U scikit-learn\n",
    "#!pip3 install -U RISE\n",
    "#!pip3 install -U matplotlib\n",
    "#!pip3 install -U numpy\n",
    "# all imports\n",
    "#!pip3 install -U tensorflow\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cceec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6: Specific Problem of Natural Language Processing\n",
    "\n",
    "## by Ziwei Chen, Stephan Nef, Lukas Bamert and Jan Grau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a89b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. Words to mathematical representation\n",
    "2. Embedding the problem into already learnt\n",
    "3. Transformer Encoder\n",
    "    1. Self-Attention\n",
    "    2. position-wise Feedforward Networks\n",
    "    3. Residucal connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e5661",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part Jan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944eabd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Transformer_Base.png\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d926b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Encoder.PNG\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6ff56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"attention.jpg\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb23dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember the problem of FIR filters? The length of the filter is too short for the actual input and quality of the data of the input may differ.\n",
    "\n",
    "There is a solution to this: $\\underline{attention}$.\n",
    "\n",
    "Let $v = [v_1,..., v_n]$ be a sequence of input vectors.\n",
    "\n",
    "Then we can define a context vector $c$ as $c= \\sum_{i=1}^n \\alpha_iv_i$.\n",
    "\n",
    "This can be extrapolated to different context vectors, each describing different contexts $j$:\n",
    "\n",
    "$$c_j = \\sum_{i=1}^n \\alpha_{ji}v_i $$\n",
    "\n",
    "\n",
    "where $\\alpha_{ji}$ is an attention weight from input $i$ to output $j$. A good way to achieve this is to use the softmax function:\n",
    "\n",
    "$$\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$$\n",
    "\n",
    "where $g_{ji}$ is using an alignment model to tell about the similarity of two vectors:\n",
    "\n",
    "$$ g_{ji} = \\frac{q'k_i}{\\sqrt{k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a17b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Attention1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057c84b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a context vector calculation in pratice\n",
    "\n",
    "# let's try to figure out the market value of newcomer _\n",
    "# given our scouting DB with current market values of known players\n",
    "\n",
    "v = {}\n",
    "v[\"messi\"] = 80\n",
    "v[\"lewandowski\"] = 40\n",
    "v[\"miller\"] = -25\n",
    "\n",
    "# since we already have an example of cosine similarity g_ji is given here\n",
    "your_player = \"ronald\"\n",
    "\n",
    "g = {}\n",
    "g[\"yp-messi\"] = 0.8\n",
    "g[\"yp-lewandowski\"] = 0.5\n",
    "g[\"yp-miller\"] = -0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffffdf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# a litte helper\n",
    "sum_eg = 0\n",
    "for mv in g:\n",
    "    sum_eg += math.exp(g[mv])\n",
    "\n",
    "# calculate attention values\n",
    "alpha_yp_messi = math.exp(g[\"yp-messi\"])/sum_eg\n",
    "alpha_yp_lewandowski = math.exp(g[\"yp-lewandowski\"])/sum_eg\n",
    "alpha_yp_maguire = math.exp(g[\"yp-miller\"])/sum_eg\n",
    "c = alpha_yp_messi *v[\"messi\"] + alpha_yp_lewandowski * v[\"lewandowski\"] + alpha_yp_maguire * v[\"miller\"]\n",
    "print(\"expected market value of \", your_player, \": \", round(c,2), \"Mio CHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takeaway\n",
    "\n",
    "We calculate basic attention by querying (your player aka $q$) to keys (Messi & co, aka $k$) to get a value (market value aka $v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb7f02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "What if we try to calculate our $q$, $k$, and $v$ by ourselves?\n",
    "\n",
    "This process is called self-attention, where each input vector from $x = [x_1,...,x_n]$ is also query, key and value:\n",
    "\n",
    "$$x_i = q_i = k_i = v_i$$\n",
    "\n",
    "However instead of just calculating $c_j = \\sum_{i=1}^n \\alpha_{ji}v_i$ with $\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$, it has been proven mathematically beneficial to linearly project these vectors (in our example we had a scalar value) into smaller dimensionalities. For this we use three projection matrices $W^Q$, $W^K$, $W^V$. This will give us the following equations:\n",
    "\n",
    "$$ q_i^\\star = W^Qq_i, k_i^\\star = W^Kk_i,v_i^\\star = W^Vv_i$$\n",
    "\n",
    "\n",
    "These $W$ play an essential role in the learning. Since the attention mechanism does not contain trainable parameters. Therefore given fixed inputs vectors, we need to learn the elements of the $W$'s. Also note that having two different matrices $W^Q$ and $W^K$ we will have asymetric relationships between the input vector elements.\n",
    "\n",
    "In the end we can calculate $c_j =  \\sum_{i=1}^n \\alpha_{ji}v_i^\\star$ for each element $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ee2c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Self-Attention1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347d03f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"SelfAttention_Context1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c54e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Self-Attention-Overall.drawio.png\" width=100% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eeca23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"Honestly, I donâ€™t understand anything anymore\", \n",
    "             \"Miller said.\",\n",
    "             \"With all due respect to Messi and the other great players named,\",\n",
    "             \"no one deserved it as much as Lewandowski.\", \n",
    "             \"To be as remarkable as the Bavarian striker.\",\n",
    "             \"Lewandowski numbers do look better on paper\"]\n",
    "\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences=texts, vector_size=8, window=5, min_count=1, workers=4)\n",
    "x_messi = model.wv['Messi']\n",
    "x_lewa = model.wv['Lewandowski']\n",
    "x_miller = model.wv['Miller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b60adb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([x_messi, x_lewa, x_miller])\n",
    "\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "\n",
    "# generate the three projections matrices\n",
    "W_Q = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_K = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_V = np.random.random_sample((d_v, d_model)) # in a trainable model, those would be trained instead of random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417439dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "c = np.zeros((x.shape[0], d_v))\n",
    "\n",
    "\n",
    "k_stars = np.array([np.dot(W_K, xi) for xi in x])\n",
    "q_stars = np.array([np.dot(W_Q, xi).transpose() for xi in x])\n",
    "v_stars = np.array([np.dot(W_V, xi) for xi in x])\n",
    "\n",
    "for j in range(x.shape[0]):\n",
    "    qj_star = q_stars[j]\n",
    "    all_gj = np.array([np.dot(qj_star, k_stars[i]) / np.sqrt(d_model) for i in range(x.shape[0])]) # 3x1\n",
    "    sum_g = np.sum(np.array([math.exp(all_gj[i]) for i in range(x.shape[0])]))\n",
    "    alpha_j = np.array([math.exp(all_gj[i]) / sum_g for i in range(x.shape[0])])\n",
    "    c[j] = np.sum([np.dot(alpha_j[i], v_stars[i]) for i in range(x.shape[0])], axis=0)\n",
    "print(c[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e60ddd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Encoder.PNG\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb66040",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "- one attention head c can capture one relationship\n",
    "\n",
    "- in a sentence there are many relationships\n",
    "\n",
    "- multiple head's whith their on $W^Q$, $W^K$ and $W^V$\n",
    "\n",
    "- additional $W^O$ to combine heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545b6df",
   "metadata": {},
   "source": [
    "<img src=\"MultiHead.svg\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f057f6",
   "metadata": {},
   "source": [
    "## Multihead Attention Math\n",
    "\n",
    "\n",
    "- $W^O$ with $ d_{model} \\times hd_v$\n",
    "\n",
    "- $W^{O}_h$ with $d_{model} \\times d_v$\n",
    "\n",
    "- $c_j$ with $1 \\times d_v$\n",
    "\n",
    "\n",
    "$c_j$ of each head has dimensionality $d_v$, will be denoted with $c_{hj}$ \n",
    "\n",
    "$$z_j = \\sum_{h=1}^H W^{O}_h c_{hj} = W^{O} \\cdot [c_{1j} ... c_{Hj}]^T  $$\n",
    "\n",
    "$z_j$ with $( d_{model} \\times hd_v) \\times (hd_v \\times 1) = d_{model} $\n",
    "\n",
    "$z$ with $ inputs \\times d_{model} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeec2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([np.random.random_sample(8) for x in range(3)])\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "h_count = 3 # Header Count\n",
    "\n",
    "W_Q = np.random.random_sample((h_count,d_q, d_model))\n",
    "W_K = np.random.random_sample((h_count,d_q, d_model))\n",
    "W_V = np.random.random_sample((h_count,d_v, d_model))\n",
    "\n",
    "\n",
    "c_jh = np.zeros((x.shape[0], h_count, d_v))\n",
    "\n",
    "for hi in range(h_count):\n",
    "    k_stars = np.array([np.dot(W_K[hi], xi) for xi in x])\n",
    "    q_stars = np.array([np.dot(W_Q[hi], xi).transpose() for xi in x])\n",
    "    v_stars = np.array([np.dot(W_V[hi], xi) for xi in x])\n",
    "    \n",
    "    for j in range(x.shape[0]):\n",
    "        qj_star = q_stars[j]\n",
    "        all_gj = np.array([np.dot(qj_star, k_stars[i]) / np.sqrt(d_model) for i in range(x.shape[0])]) # 3x1\n",
    "        sum_g = np.sum(np.array([math.exp(all_gj[i]) for i in range(x.shape[0])]))\n",
    "        alpha_j = np.array([math.exp(all_gj[i]) / sum_g for i in range(x.shape[0])])\n",
    "        c_jh[j][hi] = np.sum([np.dot(alpha_j[i], v_stars[i]) for i in range(x.shape[0])], axis=0)\n",
    "\n",
    "W_O = np.random.random_sample((x.shape[0],d_model, h_count*d_v))\n",
    "  \n",
    "z = np.zeros((x.shape[0], d_model))\n",
    "c = c_jh.reshape(x.shape[0], h_count * d_v)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(W_O[i], c[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8790ec0",
   "metadata": {},
   "source": [
    "## Pointwise Feedforward Network\n",
    "\n",
    "- each position same transformation (with same weights)\n",
    "\n",
    "- fully connected\n",
    "\n",
    "- $d_{inner} > d_{model}$\n",
    "\n",
    "- take up approx. $2/3$ of transformer parameters\n",
    "\n",
    "- might serve as key/value pair (https://arxiv.org/pdf/2012.14913.pdf)\n",
    "\n",
    "- $in = out =  d_{model}$\n",
    "\n",
    "\n",
    "$$PFF(z_j) = W_2 F(W_1 z_j + b_1 ) + b2   $$\n",
    "\n",
    "$$ F(x) = max(0,x) = Relu $$\n",
    "\n",
    "\n",
    "- So attention might not be everything you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_inner = d_model * 4\n",
    "\n",
    "W_1 = np.random.random_sample((d_inner, d_model))\n",
    "W_2 = np.random.random_sample((d_model, d_inner))\n",
    "b_1 = np.random.random_sample((d_inner))\n",
    "b_2 = np.random.random_sample((d_model))\n",
    "relu = np.zeros((d_inner))\n",
    "y = np.zeros((x.shape[0], d_model))\n",
    "for i in range(x.shape[0]):\n",
    "    hidden_layer = np.maximum(np.dot(W_1, z[i] ) + b_1, d_inner)\n",
    "    y[i] = np.dot(W_2, hidden_layer) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509cbf79",
   "metadata": {},
   "source": [
    "## Residual Connection and Layer Normalization\n",
    "\n",
    "- aim to improve the converge of optimzation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd695914",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "Substitue $y = f(x,p)$ by $y = x + g(x,q)$ where q and p are parameter vectors\n",
    "\n",
    "This means that $g(x,q) = f(x,p) - x$\n",
    "\n",
    "- $ g(x,q)$ can be easier to optimize if f is close to the identity function $id(x) = x$\n",
    "- $ q(x,q)$ learns how much the input x needs to change\n",
    "\n",
    "- if $initial weights = 0$ without residual, then $output\\approx zerofunction$\n",
    "- if $initial weights = 0$ with residual, then $output \\approx identity(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1178e2f",
   "metadata": {},
   "source": [
    "Reasoning:\n",
    "\n",
    "- gradient of objective function E ( Error function) $E(y)$.\n",
    "- Chain-Rule\n",
    "$$ E'(y) = E'(y)\\cdot y'$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} = \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x}$$\n",
    "\n",
    "Without residual: y = Px\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x} = \\frac{\\delta E}{\\delta y}P$$\n",
    "\n",
    "With residual connection: y = x + Qx, I = Identy Matrix (derivate of x)\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta y} \\frac{\\delta y}{\\delta x} = \\frac{\\delta E}{\\delta y}(I+Q)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef69766",
   "metadata": {},
   "source": [
    "### One more justification - Vanishing Gradient Problem\n",
    "\n",
    "- Deep (stacked) network with H layers\n",
    "\n",
    "$$ y_h = Ix + F_h(x) = (I + F_h)x $$\n",
    "\n",
    "- stack of layers 1,...H\n",
    "\n",
    "$$ y = ( \\prod_{h=1}^H (I + F_h) ) x $$\n",
    "\n",
    "\n",
    "$$ \\prod_{h=1}^H (I + F_h) = I + \\sum_{h\\leq H}F_h + \\sum_{i<j\\leq H}F_jF_i +... + \\prod_{h=H,..,1}F_h $$\n",
    "\n",
    "- non residual would correspond to last term:\n",
    "\n",
    "$$  \\prod_{h=H,..,1}F_h$$\n",
    "\n",
    "- size of gradient tends to decrease with chaining layers -> vanishing\n",
    "\n",
    "- residual connection contains term $\\sum_hF_h $ ( sum of outputs of invididual layers)\n",
    "\n",
    "- this prevents gradients from vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e201cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,3))\n",
    "result = layers.MultiHeadAttention(key_dim=2, num_heads=2, use_bias=False, kernel_initializer='zeros')(inputs, inputs)\n",
    "result = layers.Add()([inputs, result])\n",
    "model = keras.models.Model(inputs=inputs, outputs=result)\n",
    "test_input = tf.constant([[[1,2,3]]])\n",
    "result = model(test_input) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45990a",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "- Input variables have different scaling\n",
    "\n",
    "\n",
    "### Why is that an issue?\n",
    "\n",
    "- in a linear model scaling is accounted for by the pseudoinversion X'X\n",
    "\n",
    "- might lead to bad numerical conditioning of the inverse\n",
    "\n",
    "- in a stacked network, outputs could also hit problematic regions of nonlinear activation functions\n",
    "\n",
    "- Weight Matrices are unbound -> could also grow limitless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53969aa",
   "metadata": {},
   "source": [
    "<img src=\"BatchLayerNorm.png\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775b7f5",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- a priori knowledge of mean $m$ and variance $v$ of variable $z$ from test-set\n",
    "\n",
    "$$ \\hat{y} = \\frac{z-m}{\\sqrt{v}}$$\n",
    "\n",
    "\n",
    "- For fit measure E = MSE the gradients:\n",
    "\n",
    "$$ \\frac{\\delta E}{\\delta z} = \\frac {\\delta E}{\\delta y}\\frac{1}{\\sqrt{v}} [(1-\\frac {1}{H}) - (z-m)^2 \\frac{1}{Hv}] $$\n",
    "\n",
    "\n",
    "- bounderies for the norm of gradients and hessian matrix of second derivatives\n",
    "\n",
    "- batch normalization seems to make the mapping smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174fa19",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "- in batch normalization gradient of one sample depends on all other samples as well\n",
    "\n",
    "- layer normalization calculates mean and variance over feature dimension\n",
    "\n",
    "- Layer Normalization in Transformer: \n",
    "\n",
    "$$ \\hat{x} = \\frac{x -m}{\\sqrt{v^2}}$$\n",
    "\n",
    "Applied after Residual Connection of MultiHeadAttention and PFF:\n",
    "\n",
    ">$ z^*_j = LayerNorm(z_j + x_j)$ where $ z_j = MultiHeadAttt(x_j, x)$\n",
    "\n",
    "\n",
    ">$ y^*_j = LayerNorm(y_j + z^*_j)$ where $y_j = PFF(z^*_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "beta = random.uniform(-1, 1)\n",
    "gamma = random.uniform(-1,1)\n",
    "epsilon = 0.00001\n",
    "\n",
    "v_i = x[0]\n",
    "mu = 1 / v_i.shape[0] * np.sum(v_i)\n",
    "omega_squared = 1 / v_i.shape[0] * np.sum((v_i - mu)**2)\n",
    "Layer_Norm = gamma * (v_i - mu) / (np.sqrt(omega_squared) + epsilon) + beta\n",
    "print(Layer_Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba224e3",
   "metadata": {},
   "source": [
    "### How it is done\n",
    "\n",
    "$$ LayerNorm(x) = \\gamma \\frac{x-m}{\\sqrt{v^2} + \\epsilon}+ \\beta $$\n",
    "> $ x \\in \\mathbb{R}^{d_{model}}$\n",
    "\n",
    "> $ mean = m = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} x_i$\n",
    "\n",
    "> $ v^2 = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} (x_i - m)^2$\n",
    "\n",
    "> $ \\epsilon$ and $\\beta$ are two learnable parameters\n",
    "\n",
    "- Simply normalizing to zero mean and unity variance could constrain the inputs to a specific subset of the activation function\n",
    "\n",
    "- $\\epsilon$ and $\\beta$ can help learn the indentity function by scaling and shifting the normalized value"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
