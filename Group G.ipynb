{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c8718",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# install RISE with https://rise.readthedocs.io/en/stable/installation.html\n",
    "#!pip3 install -U scikit-learn\n",
    "#!pip3 install -U RISE\n",
    "#!pip3 install -U matplotlib\n",
    "#!pip3 install -U numpy\n",
    "# all imports\n",
    "#!pip3 install -U tensorflow\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cceec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6: Specific Problem of Natural Language Processing\n",
    "\n",
    "## by Ziwei Chen, Stephan Nef, Lukas Bamert and Jan Grau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a89b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. Words to mathematical representation\n",
    "2. Embedding the problem into already learnt\n",
    "3. Transformer Encoder\n",
    "    1. Self-Attention\n",
    "    2. position-wise Feedforward Networks\n",
    "    3. Residucal connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e5661",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part Jan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944eabd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Transformer_Base.png\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d926b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Encoder.PNG\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6ff56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"attention.jpg\" width=50% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb23dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember the problem of FIR filters? The length of the filter is too short for the actual input and quality of the data of the input may differ.\n",
    "\n",
    "There is a solution to this: $\\underline{attention}$.\n",
    "\n",
    "Let $v = [v_1,..., v_n]$ be a sequence of input vectors.\n",
    "\n",
    "Then we can define a context vector $c$ as $c= \\sum_{i=1}^n \\alpha_iv_i$.\n",
    "\n",
    "This can be extrapolated to different context vectors, each describing different contexts $j$:\n",
    "\n",
    "$$c_j = \\sum_{i=1}^n \\alpha_{ji}v_i $$\n",
    "\n",
    "\n",
    "where $\\alpha_{ji}$ is an attention weight from input $i$ to output $j$. A good way to achieve this is to use the softmax function:\n",
    "\n",
    "$$\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$$\n",
    "\n",
    "where $g_{ji}$ is using an alignment model to tell about the similarity of two vectors:\n",
    "\n",
    "$$ g_{ji} = \\frac{q'k_i}{\\sqrt{k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a17b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Attention1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057c84b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a context vector calculation in pratice\n",
    "\n",
    "# let's try to figure out the market value of newcomer _\n",
    "# given our scouting DB with current market values of known players\n",
    "\n",
    "v = {}\n",
    "v[\"messi\"] = 80\n",
    "v[\"lewandowski\"] = 40\n",
    "v[\"miller\"] = -25\n",
    "\n",
    "# since we already have an example of cosine similarity g_ji is given here\n",
    "your_player = \"ronald\"\n",
    "\n",
    "g = {}\n",
    "g[\"yp-messi\"] = 0.8\n",
    "g[\"yp-lewandowski\"] = 0.5\n",
    "g[\"yp-miller\"] = -0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffffdf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# a litte helper\n",
    "sum_eg = 0\n",
    "for mv in g:\n",
    "    sum_eg += math.exp(g[mv])\n",
    "\n",
    "# calculate attention values\n",
    "alpha_yp_messi = math.exp(g[\"yp-messi\"])/sum_eg\n",
    "alpha_yp_lewandowski = math.exp(g[\"yp-lewandowski\"])/sum_eg\n",
    "alpha_yp_maguire = math.exp(g[\"yp-miller\"])/sum_eg\n",
    "c = alpha_yp_messi *v[\"messi\"] + alpha_yp_lewandowski * v[\"lewandowski\"] + alpha_yp_maguire * v[\"miller\"]\n",
    "print(\"expected market value of \", your_player, \": \", round(c,2), \"Mio CHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takeaway\n",
    "\n",
    "We calculate basic attention by querying (your player aka $q$) to keys (Messi & co, aka $k$) to get a value (market value aka $v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb7f02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "What if we try to calculate our $q$, $k$, and $v$ by ourselves?\n",
    "\n",
    "This process is called self-attention, where each input vector from $x = [x_1,...,x_n]$ is also query, key and value:\n",
    "\n",
    "$$x_i = q_i = k_i = v_i$$\n",
    "\n",
    "However instead of just calculating $c_j = \\sum_{i=1}^n \\alpha_{ji}v_i$ with $\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$, it has been proven mathematically beneficial to linearly project these vectors (in our example we had a scalar value) into smaller dimensionalities. For this we use three projection matrices $W^Q$, $W^K$, $W^V$. This will give us the following equations:\n",
    "\n",
    "$$ q_i^\\star = W^Qq_i, k_i^\\star = W^Kk_i,v_i^\\star = W^Vv_i$$\n",
    "\n",
    "\n",
    "These $W$ play an essential role in the learning. Since the attention mechanism does not contain trainable parameters. Therefore given fixed inputs vectors, we need to learn the elements of the $W$'s. Also note that having two different matrices $W^Q$ and $W^K$ we will have asymetric relationships between the input vector elements.\n",
    "\n",
    "In the end we can calculate $c_j =  \\sum_{i=1}^n \\alpha_{ji}v_i^\\star$ for each element $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ee2c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Self-Attention1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347d03f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"SelfAttention_Context1.drawio.png\" width=80% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c54e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Self-Attention-Overall.drawio.png\" width=100% style=\"margin-left:auto; margin-right:auto\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eeca23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"Honestly, I donâ€™t understand anything anymore\", \n",
    "             \"Miller said.\",\n",
    "             \"With all due respect to Messi and the other great players named,\",\n",
    "             \"no one deserved it as much as Lewandowski.\", \n",
    "             \"To be as remarkable as the Bavarian striker.\",\n",
    "             \"Lewandowski numbers do look better on paper\"]\n",
    "\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences=texts, vector_size=8, window=5, min_count=1, workers=4)\n",
    "x_messi = model.wv['Messi']\n",
    "x_lewa = model.wv['Lewandowski']\n",
    "x_miller = model.wv['Miller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b60adb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([x_messi, x_lewa, x_miller])\n",
    "\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "\n",
    "# generate the three projections matrices\n",
    "W_Q = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_K = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_V = np.random.random_sample((d_v, d_model)) # in a trainable model, those would be trained instead of random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417439dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "c = np.zeros((x.shape[0], d_v))\n",
    "\n",
    "\n",
    "k_stars = np.array([np.dot(W_K, xi) for xi in x])\n",
    "q_stars = np.array([np.dot(W_Q, xi).transpose() for xi in x])\n",
    "v_stars = np.array([np.dot(W_V, xi) for xi in x])\n",
    "\n",
    "for j in range(x.shape[0]):\n",
    "    qj_star = q_stars[j]\n",
    "    all_gj = np.array([np.dot(qj_star, k_stars[i]) / np.sqrt(d_model) for i in range(x.shape[0])]) # 3x1\n",
    "    sum_g = np.sum(np.array([math.exp(all_gj[i]) for i in range(x.shape[0])]))\n",
    "    alpha_j = np.array([math.exp(all_gj[i]) / sum_g for i in range(x.shape[0])])\n",
    "    c[j] = np.sum([np.dot(alpha_j[i], v_stars[i]) for i in range(x.shape[0])], axis=0)\n",
    "print(c[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e60ddd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Encoder.PNG\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
