{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4c8718",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# install RISE with https://rise.readthedocs.io/en/stable/installation.html\n",
    "#!pip3 install -U scikit-learn\n",
    "# all imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cceec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6: Specific Problem of Natural Language Processing\n",
    "\n",
    "## by Ziwei Chen, Stephan Nef, Lukas Bamert and Jan Grau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a89b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. Words to mathematical representation\n",
    "2. Embedding the problem into already learnt\n",
    "3. Transformer Encoder\n",
    "    1. Self-Attention\n",
    "    2. position-wise Feedforward Networks\n",
    "    3. Residucal connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da4dee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to NLP\n",
    "give examples on application, where it is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33522ef0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Words to mathematical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478cf3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "explain the problems of word to NLP, why we need to transform the words into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5606a2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take the formula from chapter 6.1 show it with some explanation in markdown and create a code example. Example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79879797",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 0 0 0 1 0 1 0 0 0]\n",
      " [1 0 0 0 1 0 1 0 0 1 1 1 1 0 0]\n",
      " [0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
      " [0 0 1 1 0 0 0 0 1 0 0 0 0 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "          'Text of first Lukas.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three is new.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "X = vectorizer.fit_transform(corpus)# columns of X correspond to the result of this method\n",
    "vectorizer.get_feature_names_out() == (\n",
    "    ['document', 'first', 'four', 'is', 'longer',\n",
    "     'made', 'number', 'of', 'second', 'text',\n",
    "     'the', 'this', 'three', 'new', 'Lukas'])# retrieving the matrix in the numpy form\n",
    "print(X.toarray())# transforming a new document according to learn vocabulary\n",
    "vectorizer.transform(['A new Lukas is three.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d409ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "explain advantages/disadvantages of Word Embeddings as calculated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf7d06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "introduce the formula $x_i = Mt_i$ and $x_i^\\star= x_i +p_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0307e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If found, give code example for the formulas otherwise use an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef45e8a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#insert code for formula above or use an image to explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc66728",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "explain formula $w_{AB} = v_A'v_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecda2c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# code example of formula above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f29547",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embed previous learnt methods to NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a510c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### modelling it as FIR\n",
    "- draw some relations back to FIR with an example and image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552ca05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### modelling it as IIR\n",
    "- draw some relations back to IIR with an example and image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c7f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### modelling it as RNN\n",
    "- draw some relations back to discrete linear systems with an example and image\n",
    "- introduce the new formula $h_t = H(h_{t-1}, x_{t-1})$ and $y_t = Y(h_t,x_t)$\n",
    "- explain the differences between classical RNNs and our application\n",
    "- also with the new formula $h_t = H(h_{t-1}, x_{t})$ and $y_t = Y(h_t)$\n",
    "- explain the vanishing gradient problem in RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a9ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## from RNN to LSTMs\n",
    "- introduce the state formula $c_t = C(h_{t-1}, x_t, c_{t-1})$\n",
    "- introduce formula 6.13\n",
    "- introduce the three gates (forget gate, input gate and output gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683c8855",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# code to show the Hadamard operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c364767",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# code to explain the gates (if easily found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e5661",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### improvements for RNNs\n",
    "- bidirectional sequencing\n",
    "- example with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6ff56",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![Attention is all you need](attention.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb23dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attention\n",
    "\n",
    "Remember the problem of FIR filters? The length of the filter is too short for the actual input and quality of the data of the input may differ.\n",
    "\n",
    "There is a solution to this: $\\underline{attention}$.\n",
    "\n",
    "Let $v = [v_1,..., v_n]$ be a sequence of input vectors.\n",
    "\n",
    "Then we can define a context vector $c$ as $c= \\sum_{i=1}^n \\alpha_iv_i$.\n",
    "\n",
    "This can be extrapolated to different context vectors, each describing different contexts $j$:\n",
    "\n",
    "$$c_j = \\sum_{i=1}^n \\alpha_{ji}v_i $$\n",
    "\n",
    "\n",
    "where $\\alpha_{ji}$ is an attention weight from input $i$ to output $j$. A good way to achieve this is to use the softmax function:\n",
    "\n",
    "$$\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$$\n",
    "\n",
    "where $g_{ji}$ is using an alignment model to tell about the similarity of two vectors:\n",
    "\n",
    "$$ g_{ji} = \\frac{q_j'k_i}{\\sqrt{k}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0057c84b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a context vector calculation in pratice\n",
    "\n",
    "# let's try to figure out the market value of newcomer Güven \n",
    "# given our scouting DB with current market values of known players\n",
    "\n",
    "v = {}\n",
    "v[\"messi\"] = 80\n",
    "v[\"lewandowski\"] = 40\n",
    "v[\"maguire\"] = -25\n",
    "\n",
    "# since we already have an example of cosine similarity g_ji is given here\n",
    "\n",
    "g = {}\n",
    "g[\"güven-messi\"] = 0.8\n",
    "g[\"güven-lewandowski\"] = 0.5\n",
    "g[\"güven-maguire\"] = -0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffffdf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected market value of Güven:  53.83 Mio CHF\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# a litte helper\n",
    "sum_eg = 0\n",
    "for mv in g:\n",
    "    sum_eg += math.exp(g[mv])\n",
    "\n",
    "# calculate attention values\n",
    "alpha_güven_messi = math.exp(g[\"güven-messi\"])/sum_eg\n",
    "alpha_güven_lewandowski = math.exp(g[\"güven-lewandowski\"])/sum_eg\n",
    "alpha_güven_maguire = math.exp(g[\"güven-maguire\"])/sum_eg\n",
    "c = alpha_güven_messi *v[\"messi\"] + alpha_güven_lewandowski * v[\"lewandowski\"] + alpha_güven_maguire * v[\"maguire\"]\n",
    "print(\"expected market value of Güven: \", round(c,2), \"Mio CHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takeaway\n",
    "\n",
    "We calculate basic attention by querying (Güven aka $q$) to keys (Messi & co, aka $k$) to get a value (market value aka $v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c56e2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autocoding out of context\n",
    "\n",
    "Tell something about chapter 6.6. But probably after transformers are explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb7f02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "What if we try to calculate our $q$, $k$, and $v$ by ourselves?\n",
    "\n",
    "This process is called self-attention, where each input vector from $x = [x_1,...,x_n]$ is also query, key and value:\n",
    "\n",
    "$$x_i = q_i = k_i = v_i$$\n",
    "\n",
    "However instead of just calculating $c_j = \\sum_{i=1}^n \\alpha_{ji}v_i$ with $\\alpha_{ji} = \\frac{e^{g_{ji}}}{\\sum_{k=1}^ne^{g_{jk}}}$, it has been proven mathematically beneficial to linearly project these vectors (in our example we had a scalar value) into smaller dimensionalities. For this we use three projection matrices $W^Q$, $W^K$, $W^V$. This will give us the following equations:\n",
    "\n",
    "$$ q_i^\\star = W^Qq_i, k_i^\\star = W^Kk_i,v_i^\\star = W^Vv_i$$\n",
    "\n",
    "\n",
    "These $W$ play an essential role in the learning. Since the attention mechanism does not contain trainable parameters. Therefore given fixed inputs vectors, we need to learn the elements of the $W$'s. Also note that having two different matrices $W^Q$ and $W^K$ we will have asymetric relationships between the input vector elements.\n",
    "\n",
    "In the end we can calculate $c_j =  \\sum_{i=1}^n \\alpha_{ji}v_i^\\star$ for each element $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b60adb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# input vector (maybe take values from Stephan/Ziwei)\n",
    "x = np.array([np.random.random_sample(8) for x in range(3)])\n",
    "\n",
    "# we need to set the dimensions\n",
    "d_model = len(x[0]) # always the length of the input vectors\n",
    "d_q = d_model // 4 # theoretically freely choosable to linear transform the projection matrix\n",
    "d_v = d_model // 2 # can be different for the values, but usually not\n",
    "\n",
    "# generate the three projections matrices\n",
    "W_Q = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_K = np.random.random_sample((d_q, d_model)) # in a trainable model, those would be trained instead of random\n",
    "W_V = np.random.random_sample((d_v, d_model)) # in a trainable model, those would be trained instead of random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417439dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.3480872  5.96059901 4.44070195 4.14138009]\n"
     ]
    }
   ],
   "source": [
    "# calculate context vector c_1 (based on x_1)\n",
    "c_1 = 0\n",
    "\n",
    "# iterate over each and every element in input vector\n",
    "for i in range(len(x)):\n",
    "    # query is always x_1\n",
    "    q_star = np.matmul(W_Q, x[0])\n",
    "    k_star = np.matmul(W_K, x[i])\n",
    "    g_ji = (np.dot(q_star.transpose(),k_star)) / len(W_K[0])\n",
    "    # for alpha we need the sum of e^g^jk\n",
    "    e_g_jk = 0\n",
    "    for i in range(len(x)):\n",
    "        g_jk = (np.dot(np.dot(W_Q, x[0]).transpose(),np.dot(W_K, x[i]))) / len(W_K[0])\n",
    "        e_g_jk += g_jk\n",
    "    alpha = math.exp(g_ji) / e_g_jk \n",
    "    v_i_star = np.matmul(W_V, x[i])\n",
    "    c_1 += np.dot(alpha,v_i_star) \n",
    "print(c_1) # c should be the same size as the value vector"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
